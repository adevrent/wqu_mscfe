{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. High Dimensional Data and Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **High dimensional data** is a dataset in which <u> the number of variables is larger than the number of observations. </u> If a dataset has $p$ variables and $n$ observations and $p > n$, then this dataset is high dimensional data. In machine learning literature, independent variables are also called **predictors,** **covariates**, **attributes** or **features**. In this course, we will refer to them as **independent variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Module 1, we talked about using principal component analysis or factor analysis, two data dimension reduction tools to solve high dimensional data issues. You first run either PCA or factor analysis to decide what variables to keep and run the linear regression model after you decide on the independent variables from PCA or factor analysis. \n",
    "\n",
    "In this lesson, we will introduce a different general method to handle the problem of too many variables: **penalized regression.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have too many variables and use all of them to build a linear regression model, we run the risk of **overfitting** the model. In an OLS regression model, we try to minimize **residual sum of squares (RSS)**, which the following objective function:\n",
    "\n",
    "$$RSS(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta _{1} X_{1i} - \\cdots - \\beta_{p}X_{pi})^{2} $$\n",
    "\n",
    "In the OLS objective function, we can see there is no restriction on how many $\\beta$ (also independent variables) there are in the function. What if we assign some sort of penalty to each additional $\\beta$ every time we add one new independent variable? Penalized regression uses an objective function like the following:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta _{1} X_{1i} - \\cdots - \\beta_{p} X_{pi})^{2} + \\lambda \\sum_{j=1}^{p} f(\\beta_j) $$\n",
    "\n",
    "where $f(.)$ is called a **regularization function** or **penalty function**. A **penalty function** will try to pull the value of a coefficient close to $0$ if the value is too large. <u> The function basically tries to reduce the impact of an independent variable in the model if the coefficient of this independent variable is too large. </u> It is a function that penalizes the existence of an independent variable in a model by suppressing the value of its coefficient. In an extreme case, this function may pull the coefficient to $0$. When the coefficient is $0$, it means the independent variable is removed from the model. The action of pulling a coefficient value towards $0$ or to a certain preset value is called **shrinkage**, in that you shrink a coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is $\\lambda$? \n",
    "\n",
    "$\\lambda$ is a **regularization parameter**. It controls the **trade-off of variance and bias** for coefficients. What does this mean? In an ideal model, we would like to achieve an accuracy for the estimated coefficients that is as high as possible and standard errors as low as possible for such estimated coefficients.\n",
    "\n",
    "In OLS, the estimated coefficients are unbiased, which means they are **accurate**. However, the variance of the coefficients can be very high if there are too many variables in the model, as we argued in the previous section. <u> An **OLS** objective function provides **accurate** (unbiased) coefficient estimates but not **stable** estimates (high variance). </u> On the other hand, a **penalty function** can suppress the coefficient estimates from going too far away from $0$. We can see from the penalized regression objective function that there is the original OLS objective function as well as a penalty function. An OLS objective function gives accurate coefficient estimates with high variance. <u> With the addition of the penalty function to the penalized regression objective function, this objective function will produce coefficient estimates with **lower variance**. However, the coefficient estimates from penalized regression will be **biased**. </u>\n",
    "\n",
    "Why does a penalized regression objective function produce biased coefficient estimates? Because <u> its ability to pull coefficient estimates closer to $0$ will prevent the model from fully minimizing the sum of the distances from the regression line to all data points. </u> This is also the reason we have the penalty function in the penalized regression objective function. We don't want to put too much emphasis on the noisy part of the data.\n",
    "\n",
    "So how can we balance the trade-off between bias and variance of the estimated coefficients? We use $\\lambda$ to balance the two forces. We can use a graph to illustrate the impact of the value of $\\lambda$ on total error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2: Trade-Off Graph between Variance and Bias in Penalized Regression**\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Figure 2, we can see that as $\\lambda$ goes to $0$, the error from variance gets larger and the error from bias gets smaller and vice versa. We would like to find the middle point of $\\lambda$ that will give us the lowest total error. So how can we find the optimal $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to discuss how to find the optimal $\\lambda$, there are steps we need to take to prepare the dataset before we model. \n",
    "\n",
    "* <u> The first thing is that the penalty function shouldn't include the intercept, $\\beta_{0}$. In general, we don't penalize $\\beta_{0}$. </u>\n",
    "\n",
    "* Note that, in contrast to the ordinary least square regression, ridge regression is highly affected by the scale of the predictors. Therefore, it is better to standardize (i.e., scale) the predictors before applying the ridge regression (James et al. 2014), so that all the predictors are on the same scale. Thus, another important step to take is that of <u> standardizing all independent variables $\\large(\\frac{x_i}{\\sqrt{Var(X_i)}})$. </u> We talked about how the penalty function will pull the value of a coefficient closer to $0$ if the coefficient value is too large. In order for this action to make sense, all independent variables need to be standardized <u> to remove the fact that they could have different **scales**. </u>\n",
    "\n",
    "Some statistical software packages may conduct independent variable standardization automatically while running a penalized regression. Please check the manual for the specific software package you decide to use for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2.4 K-Fold Cross Validation to Find Optimal $\\lambda$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we find the optimal $\\lambda$? We can use a method called **k-fold cross validation**. You can choose any number for $k$. Usually, 5 and 10 are popular numbers for $k$. Let's use 5-fold cross validation as an example to explain how it works.\n",
    "\n",
    "Let's assume that $\\lambda$ is a value between $0$ and $20$. We will try each of the following $\\lambda$: (0, 0.25, 0.5, 0.75, ... , 19.75, 20). <u> There will be a total of 80 values to try for $\\lambda$. </u>\n",
    "\n",
    "The next thing we need to do is to divide our dataset equally into <u> five groups for 5-fold cross validation. </u>\n",
    "\n",
    "Say that our dataset contains 100 observations. <u> Then, we will create **5 groups** each containing **20 observations**. </u> Let's call our 5 groups $A$, $B$, $C$, $D$, and $E$ where $n=20$ :\n",
    "\n",
    "* Group $A$ contains observations $x_1, x_2, ..., x_{20}$\n",
    "* Group $B$ contains observations $x_{21}, x_{22}, ..., x_{40}$\n",
    "* Group $C$ contains observations $x_{41}, x_{42}, ..., x_{60}$\n",
    "* Group $D$ contains observations $x_{61}, x_{62}, ..., x_{80}$\n",
    "* Group $E$ contains observations $x_{81}, x_{82}, ..., x_{100}$\n",
    "\n",
    "\n",
    "We also have the penalized regression objective function:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta _{1} X_{1i} - \\cdots - \\beta _{p} X_{pi})^{2} + \\lambda \\sum_{j=1}^{p} f(\\beta_j) $$\n",
    "\n",
    "Now, we have our possible $\\lambda$ values to test, five equal groups from our data, and a penalized regression objective function. We are ready to run iterations to generate optimal $\\lambda$.\n",
    "\n",
    "Here are the steps to generate optimal $\\lambda$:\n",
    "\n",
    "1. Choose $\\lambda = 0$\n",
    "2. Choose A as the **testing dataset** and B, C, D, and E together as the **training dataset**.\n",
    "3. Build a penalized regression model using the training dataset. (because training dataset is $B+C+D+E$, the model will use $x_{21}, x_{22}, ..., x_{100}$ as observations.)\n",
    "4. Plug testing dataset A into the fitted model from step 3 and calculate the sum of residuals squared (**RSS**) from the fitted model. <u> Keep the sum of squared residuals. </u>\n",
    "5. Go back to step 2, and this time, select the next group, B, to be the testing dataset and A, C, D, and E together as the training dataset. Now, repeat steps 3, 4, and 5 until each of the five groups has been treated as a testing dataset.\n",
    "6. Calculate the average sum of squared residuals from the recorded five values of the sum of squared residuals from each of the five different testing datasets.\n",
    "7. <u> Keep a record of $\\lambda$ and its associated **average** sum of squared residuals. </u>\n",
    "8. Repeat from Step 1 to Step 7 to go through each possible value of $\\lambda$\n",
    "9. Now, you have 80 data points of $\\lambda$ and its associated average sum of squared residuals. You can find the optimal $\\lambda$ that gives the minimal value of the average sum of squared residuals. The optimal $\\lambda$ will be used for the final penalized regression.\n",
    "\n",
    "This is how 5-fold cross validation works. The same logic applies to $k$-fold cross validations. You equally divide the dataset into $k$ groups and follow the steps above to get the optimal $\\lambda$. \n",
    "\n",
    "Most of the statistical packages can conduct $k$-fold cross validation for you so you don't have to do it manually. Some software will even choose optimal $\\lambda$ for you without any specification about the range to test for $\\lambda$. \n",
    "\n",
    "Finding a good $\\lambda$ is critical to running a penalized regression. Since we now know how to obtain $\\lambda$, we can start to look at various types of penalized regression methods. In the following sections, we will learn ridge regression and lasso regression. These two regressions are widely used penalized regression methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Ridge Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty function for ridge regression is the sum of squared coefficients (betas). The mathematical formula is as follows:\n",
    "\n",
    "$$ f(\\beta) = \\lambda \\sum_{j=1}^{p}  \\beta _{j}^{2} = \\lambda \\| \\beta \\|^{2} $$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\lambda :$ regularization parameter\n",
    "\n",
    "$ \\| \\beta \\| : $ length of the vector $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above penalty function is also called an **L2 penalty function**. It is because the function is the square of the magnitude of coefficients in the model.\n",
    "\n",
    "Here are some key properties of ridge regression:\n",
    "\n",
    "1. Ridge regression **shrinks** the coefficients of the independent variables that don't contribute very much to the model <u> close to $0$, but not $0$. </u>\n",
    "2. If $\\lambda = 0$, then ridge regression estimation is **OLS** estimation.\n",
    "3. All the independent variables are still in the model.\n",
    "4. Ridge regression is good for <u> a model **without many independent variables** while the independent variables have some minor **multicollinearity** issues. </u> What ridge regression does is restrict coefficients of correlated independent variables and make their coefficients similar. When you want to keep all variables but also want to mitigate the multicollinearity issue, ridge regression is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Lasso Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso regression** is short for **least absolute shrinkage and selection operator**. Lasso regression has the following penalty function:\n",
    "\n",
    "$$ f(\\beta) = \\lambda \\sum_{j=1}^{p} |\\beta _{j}| = \\lambda \\| \\beta \\| $$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\lambda :$ regularization parameter\n",
    "\n",
    "$ \\| \\beta \\| : $ length of the vector $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above penalty function is also called an **L1 penalty function**. It is because the function is the absolute value of the magnitude of coefficients in the model.\n",
    "\n",
    "Here are some key properties of lasso regression:\n",
    "\n",
    "1. Lasso regression can shrink the coefficients of the independent variables that don't contribute very much to the model to $0$. Hence, <u> lasso regression can **remove** independent variables. </u> Lasso regression can be used for **variable selection.**\n",
    "2. If $\\lambda = 0$, then a lasso regression estimation is an **OLS** estimation.\n",
    "3. Lasso regression is a good choice when you have <u> **too many independent variables** and you are looking to select only key variables for the model. </u> Lasso regression can first group independent variables with high correlations together, choose only one of them for the model, and drop the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. An Example: Foreign Exchange Market and Penalized Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependent Variable:\n",
    "\n",
    "**DXY**: U.S. Dollar Index daily return\n",
    "\n",
    "Independent Variables:\n",
    "\n",
    "- **METALS**: Gold and Silver Index daily return  <br>\n",
    "- **US_STK**: S\\&P 500 Index daily return <br>\n",
    "- **X13W_TB**: 13-week Treasury Bills daily return <br>\n",
    "- **X10Y_TBY**: 10 Year Treasury Bond Yield daily return <br>\n",
    "- **EURUSD**: EURUSD daily return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split our data into 80% training set, 20% test set. Or use **K-fold cross-validation** and split our observations into $k$ groups, and start an **iteration** where we first take the group $A_1$ as the test dataset and the groups $A_2, ..., A_k$ as training datasets. Then use all different $\\lambda$ values for each test-training pair, calculate the average $RSS$ for each value of $\\lambda$ then decide to choose the $\\lambda$ value where the $RSS$ is the lowest. Then run **OLS, Ridge, Lasso** regressions and get $R^2$ values to examine them. Here are the steps:\n",
    "\n",
    "**1.** We first run an **OLS** regression model on the training dataset and get coefficients estimate $\\hat{\\beta}$. Then we plug the **test dataset** into our model with coefficients estimate $\\hat{\\beta}$ and save the $R^2$ value.\n",
    "\n",
    "**2.** We choose a range for regularizing constant $\\lambda$. For example let's say $\\lambda \\in [0, 20]$. Then, we split the range into $n$ lambda values, let's say we have $\\lambda = 0.0, 0.5, 0.75, 1.00, ..., 20.0$\n",
    "\n",
    "**2.** **Choosing $\\lambda$:** After fitting OLS, we can actually use **K-fold cross-validation** to find an optimal $\\lambda$ value for **ridge and lasso regressions**. Here's how on K-fold cross-validation: We start with $\\lambda = 0$. We split our observations into $k$ groups, and start an **iteration** where we first take the group $A_1$ as the test dataset and the groups $A_2, ..., A_k$ as training datasets. Then we fit a **ridge** or **lasso** regression and store the resulting $RSS$ value. Then we take $A_2 as the test dataset and the groups $A_1, A_3, ..., A_k$ as the training dataset, fit the regression and store the $RSS$ value again. Then when all groups are used as test datasets, we take the average of all $k$ $RSS$ values and store that average value. Then, we increment our lambda value, let's say $\\lambda=0.5$ and do the whole process again until all range of $\\lambda$ is exhausted. Then we examine the average $RSS$ values and pick the value of $\\lambda$ where the average $RSS$ is the lowest.\n",
    "\n",
    "**3.** After choosing our ideal $\\lambda$, we fit a **ridge regression** model and save the $R^2$ value.\n",
    "\n",
    "**4.** Do the steps **2.** and **3.** for **lasso regression**.\n",
    "\n",
    "For the sake of simplicity, we'll use library functions for $k$ fold cross validation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DXY</th>\n",
       "      <th>METALS</th>\n",
       "      <th>OIL</th>\n",
       "      <th>US_STK</th>\n",
       "      <th>INTL_STK</th>\n",
       "      <th>X13W_TB</th>\n",
       "      <th>X10Y_TBY</th>\n",
       "      <th>EURUSD</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/4/2016</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>-0.007559</td>\n",
       "      <td>-0.013980</td>\n",
       "      <td>-0.019802</td>\n",
       "      <td>0.047297</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/5/2016</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.021491</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>-0.002436</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2016</td>\n",
       "      <td>-0.002213</td>\n",
       "      <td>0.013642</td>\n",
       "      <td>-0.055602</td>\n",
       "      <td>-0.012614</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031584</td>\n",
       "      <td>-0.006978</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2016</td>\n",
       "      <td>-0.009679</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>-0.020606</td>\n",
       "      <td>-0.023992</td>\n",
       "      <td>-0.019255</td>\n",
       "      <td>-0.073171</td>\n",
       "      <td>-0.011024</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2016</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>-0.028064</td>\n",
       "      <td>-0.003306</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>-0.010471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010683</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/11/2016</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>-0.057749</td>\n",
       "      <td>-0.052774</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/12/2016</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>-0.034475</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.025950</td>\n",
       "      <td>-0.007590</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/13/2016</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>-0.024941</td>\n",
       "      <td>-0.010137</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.017127</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/14/2016</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>-0.035540</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.016417</td>\n",
       "      <td>0.005788</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/15/2016</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.002409</td>\n",
       "      <td>-0.057051</td>\n",
       "      <td>-0.021466</td>\n",
       "      <td>-0.030544</td>\n",
       "      <td>-0.070833</td>\n",
       "      <td>-0.030982</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       DXY    METALS       OIL    US_STK  INTL_STK   X13W_TB  \\\n",
       "0   1/4/2016  0.002433  0.024283 -0.007559 -0.013980 -0.019802  0.047297   \n",
       "1   1/5/2016  0.005361 -0.004741 -0.021491  0.001691 -0.001263  0.322581   \n",
       "2   1/6/2016 -0.002213  0.013642 -0.055602 -0.012614 -0.015171  0.000000   \n",
       "3   1/7/2016 -0.009679  0.035249 -0.020606 -0.023992 -0.019255 -0.073171   \n",
       "4   1/8/2016  0.003258 -0.028064 -0.003306 -0.010977 -0.010471  0.000000   \n",
       "5  1/11/2016  0.001928 -0.057749 -0.052774  0.000990 -0.003527 -0.078947   \n",
       "6  1/12/2016  0.002431 -0.034475 -0.030882  0.008069  0.003982  0.200000   \n",
       "7  1/13/2016 -0.000404  0.004667  0.001314 -0.024941 -0.010137  0.023810   \n",
       "8  1/14/2016  0.001617 -0.035540  0.023622  0.016417  0.005788  0.116279   \n",
       "9  1/15/2016 -0.001312 -0.002409 -0.057051 -0.021466 -0.030544 -0.070833   \n",
       "\n",
       "   X10Y_TBY    EURUSD  YEAR  \n",
       "0 -0.010577 -0.007316  2016  \n",
       "1  0.001336 -0.002436  2016  \n",
       "2 -0.031584 -0.006978  2016  \n",
       "3 -0.011024  0.002512  2016  \n",
       "4 -0.010683  0.013636  2016  \n",
       "5  0.013146  0.001378  2016  \n",
       "6 -0.025950 -0.007590  2016  \n",
       "7 -0.017127 -0.001290  2016  \n",
       "8  0.015489  0.003875  2016  \n",
       "9 -0.030982 -0.002487  2016  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_r_working = pd.read_csv(\"/home/sscf/wqu_mscfe/Financial Econometrics/M2/fin_data_set.csv\")\n",
    "data_set_r_working.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data preparation**<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrow = 250\n",
      "training set indices = [0, 1, 4, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 60, 62, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 91, 92, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 119, 120, 122, 123, 125, 126, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 163, 164, 165, 167, 168, 170, 171, 172, 173, 174, 175, 177, 178, 179, 182, 183, 184, 185, 186, 187, 188, 189, 190, 194, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 226, 227, 228, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249]\n",
      "test set indices     = [2, 3, 5, 10, 11, 25, 28, 36, 38, 47, 55, 59, 61, 63, 71, 72, 81, 89, 90, 93, 101, 102, 116, 118, 121, 124, 132, 133, 141, 146, 160, 161, 162, 166, 169, 176, 180, 181, 191, 192, 193, 195, 196, 197, 201, 224, 225, 229, 232, 246]\n",
      "  Indices represent the i th observation. (starting from 0)\n"
     ]
    }
   ],
   "source": [
    "# Create Training Dataset and Testing Dataset\n",
    "data_set_r_working = pd.read_csv(\"/home/sscf/wqu_mscfe/Financial Econometrics/M2/fin_data_set.csv\")\n",
    "\n",
    "np.random.seed(11111)  # Random seed\n",
    "nrow = data_set_r_working.shape[0]  # number of rows, this is used to know how many observations are available.\n",
    "print(\"nrow =\", nrow)\n",
    "\n",
    "train_sequence = sorted(np.random.choice(nrow, int(nrow * 0.8), replace=False))  # randomly select 80% of the indices out of nrow = 250 indices and store it as training set indices.\n",
    "test_sequence = sorted(set(list(range(0, nrow))) - set(train_sequence))  # substract the training set indices from the nrow = 250 indices, thus resulting in test set indices.\n",
    "print(\"training set indices =\", train_sequence)\n",
    "print(\"test set indices     =\", test_sequence)\n",
    "print(\"  Indices represent the i th observation. (starting from 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **train_sequence** is a randomly selected subset of **indices**, <u> representing 80% of the dataset, used for training the model. </u>\n",
    "\n",
    "* **test_sequence** is created by subtracting train_sequence from the set of all **indices**, <u> resulting in the remaining 20% used for testing the model. </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DXY</th>\n",
       "      <th>METALS</th>\n",
       "      <th>OIL</th>\n",
       "      <th>US_STK</th>\n",
       "      <th>INTL_STK</th>\n",
       "      <th>X13W_TB</th>\n",
       "      <th>X10Y_TBY</th>\n",
       "      <th>EURUSD</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/4/2016</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>-0.007559</td>\n",
       "      <td>-0.013980</td>\n",
       "      <td>-0.019802</td>\n",
       "      <td>0.047297</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/5/2016</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.021491</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>-0.002436</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2016</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>-0.028064</td>\n",
       "      <td>-0.003306</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>-0.010471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010683</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/12/2016</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>-0.034475</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.025950</td>\n",
       "      <td>-0.007590</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/13/2016</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>-0.024941</td>\n",
       "      <td>-0.010137</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.017127</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/14/2016</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>-0.035540</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.016417</td>\n",
       "      <td>0.005788</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/15/2016</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.002409</td>\n",
       "      <td>-0.057051</td>\n",
       "      <td>-0.021466</td>\n",
       "      <td>-0.030544</td>\n",
       "      <td>-0.070833</td>\n",
       "      <td>-0.030982</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/21/2016</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>0.012968</td>\n",
       "      <td>0.112241</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>-0.003296</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/22/2016</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.090078</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/25/2016</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>0.028592</td>\n",
       "      <td>-0.057471</td>\n",
       "      <td>-0.015116</td>\n",
       "      <td>-0.009434</td>\n",
       "      <td>-0.045139</td>\n",
       "      <td>-0.012695</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       DXY    METALS       OIL    US_STK  INTL_STK   X13W_TB  \\\n",
       "0    1/4/2016  0.002433  0.024283 -0.007559 -0.013980 -0.019802  0.047297   \n",
       "1    1/5/2016  0.005361 -0.004741 -0.021491  0.001691 -0.001263  0.322581   \n",
       "4    1/8/2016  0.003258 -0.028064 -0.003306 -0.010977 -0.010471  0.000000   \n",
       "6   1/12/2016  0.002431 -0.034475 -0.030882  0.008069  0.003982  0.200000   \n",
       "7   1/13/2016 -0.000404  0.004667  0.001314 -0.024941 -0.010137  0.023810   \n",
       "8   1/14/2016  0.001617 -0.035540  0.023622  0.016417  0.005788  0.116279   \n",
       "9   1/15/2016 -0.001312 -0.002409 -0.057051 -0.021466 -0.030544 -0.070833   \n",
       "12  1/21/2016 -0.000303  0.012968  0.112241  0.005602  0.002787  0.079051   \n",
       "13  1/22/2016  0.005148  0.007385  0.090078  0.020515  0.031033  0.054945   \n",
       "14  1/25/2016 -0.002109  0.028592 -0.057471 -0.015116 -0.009434 -0.045139   \n",
       "\n",
       "    X10Y_TBY    EURUSD  YEAR  \n",
       "0  -0.010577 -0.007316  2016  \n",
       "1   0.001336 -0.002436  2016  \n",
       "4  -0.010683  0.013636  2016  \n",
       "6  -0.025950 -0.007590  2016  \n",
       "7  -0.017127 -0.001290  2016  \n",
       "8   0.015489  0.003875  2016  \n",
       "9  -0.030982 -0.002487  2016  \n",
       "12  0.017641 -0.003296  2016  \n",
       "13  0.014364 -0.001379  2016  \n",
       "14 -0.012695 -0.005799  2016  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_set_r_working.filter(items=train_sequence, axis=0)  # filter with respect to indices of rows\n",
    "test = data_set_r_working.filter(items=test_sequence, axis=0)  # filter with respect to indices of rows\n",
    "\n",
    "# Make sure X matrix is in matrix form and Y is in vector form\n",
    "ind_var = [\"METALS\", \"US_STK\", \"X10Y_TBY\", \"X13W_TB\", \"EURUSD\"]  # Choose independent (predictor) variables\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The DataFrame **train** is created by filtering **data_set_r_working** for the indices in **train_sequence**.\n",
    "\n",
    "* Similarly, **test** is created for indices in **test_sequence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>METALS</th>\n",
       "      <th>US_STK</th>\n",
       "      <th>X10Y_TBY</th>\n",
       "      <th>X13W_TB</th>\n",
       "      <th>EURUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>-0.013980</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>0.047297</td>\n",
       "      <td>-0.007316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>-0.002436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.028064</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>-0.010683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.034475</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>-0.025950</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>-0.024941</td>\n",
       "      <td>-0.017127</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.035540</td>\n",
       "      <td>0.016417</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.003875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.002409</td>\n",
       "      <td>-0.021466</td>\n",
       "      <td>-0.030982</td>\n",
       "      <td>-0.070833</td>\n",
       "      <td>-0.002487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012968</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>-0.003296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>-0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028592</td>\n",
       "      <td>-0.015116</td>\n",
       "      <td>-0.012695</td>\n",
       "      <td>-0.045139</td>\n",
       "      <td>-0.005799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    const    METALS    US_STK  X10Y_TBY   X13W_TB    EURUSD\n",
       "0     1.0  0.024283 -0.013980 -0.010577  0.047297 -0.007316\n",
       "1     1.0 -0.004741  0.001691  0.001336  0.322581 -0.002436\n",
       "4     1.0 -0.028064 -0.010977 -0.010683  0.000000  0.013636\n",
       "6     1.0 -0.034475  0.008069 -0.025950  0.200000 -0.007590\n",
       "7     1.0  0.004667 -0.024941 -0.017127  0.023810 -0.001290\n",
       "8     1.0 -0.035540  0.016417  0.015489  0.116279  0.003875\n",
       "9     1.0 -0.002409 -0.021466 -0.030982 -0.070833 -0.002487\n",
       "12    1.0  0.012968  0.005602  0.017641  0.079051 -0.003296\n",
       "13    1.0  0.007385  0.020515  0.014364  0.054945 -0.001379\n",
       "14    1.0  0.028592 -0.015116 -0.012695 -0.045139 -0.005799"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train.loc[:, ind_var]\n",
    "train_y = train.DXY\n",
    "\n",
    "test_x = test.loc[:, ind_var]\n",
    "test_y = test.DXY\n",
    "\n",
    "(sm.add_constant(train_x)).head(10)  # add a constant column of 1's as a value for beta_0  (the intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **train_x** and **test_x** are *matrices* of the training and testing sets, respectively, containing *only* the independent variables in **ind_var**.\n",
    "\n",
    "* **train_y** and **test_y** are *vectors* containing *only* the dependent variable **DXY** from the training and testing sets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tot = test.loc[:, [\"DXY\", \"METALS\", \"US_STK\", \"X10Y_TBY\", \"X13W_TB\", \"EURUSD\"]]  # test total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **test_tot** is a *DataFrame* containing both the independent variables and the dependent variable for the testing set, which might be used for later analysis or validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OLS Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Coef.  Std.Err.         t         P>|t|    [0.025    0.975]\n",
      "const     0.000410  0.000310  1.321022  1.880502e-01 -0.000202  0.001022\n",
      "METALS   -0.070259  0.010999 -6.387663  1.219474e-09 -0.091952 -0.048566\n",
      "US_STK   -0.058324  0.040419 -1.442997  1.506340e-01 -0.138040  0.021392\n",
      "X10Y_TBY  0.019521  0.013955  1.398854  1.634535e-01 -0.008002  0.047045\n",
      "X13W_TB   0.000696  0.003924  0.177426  8.593591e-01 -0.007044  0.008436\n",
      "EURUSD   -0.087070  0.059863 -1.454492  1.474259e-01 -0.205135  0.030995\n",
      "\n",
      " OLS_R^2 0.10633201319436225\n",
      " OLS_MSE 0.00381807095301603\n"
     ]
    }
   ],
   "source": [
    "# OLS Regression\n",
    "ols_final = sm.OLS(train_y, sm.add_constant(train_x)).fit()\n",
    "print(ols_final.summary2().tables[1])  # print only coefficients\n",
    "\n",
    "# Compute test R^2 and test mean squared error\n",
    "ols_pred = ols_final.predict(sm.add_constant(test_x))\n",
    "ols_pred = pd.DataFrame(ols_pred, columns=[\"ols_p\"])\n",
    "ols_actual = test.DXY\n",
    "\n",
    "ols_rss = np.sum(np.power(ols_pred.ols_p - ols_actual, 2))\n",
    "ols_tss = np.sum(np.power(ols_actual - np.mean(ols_actual), 2))\n",
    "ols_rsq = 1 - (ols_rss / ols_tss)\n",
    "print(\"\\n OLS_R^2\", ols_rsq)\n",
    "\n",
    "ols_MSE = np.sqrt(ols_rss / test.shape[0])\n",
    "print(\" OLS_MSE\", ols_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambdas = [0.0001, 0.00012589254117941674, 0.00015848931924611142, 0.0001995262314968881, 0.0002511886431509582, 0.00031622776601683826, 0.00039810717055349773, 0.000501187233627273, 0.0006309573444801943, 0.0007943282347242829, 0.001000000000000002, 0.00125892541179417, 0.0015848931924611173, 0.001995262314968885, 0.0025118864315095872, 0.003162277660168389, 0.0039810717055349856, 0.00501187233627274, 0.006309573444801955, 0.007943282347242847, 0.01000000000000004, 0.012589254117941727, 0.015848931924611207, 0.01995262314968889, 0.025118864315095923, 0.03162277660168396, 0.039810717055349935, 0.050118723362727505, 0.06309573444801969, 0.07943282347242862, 0.10000000000000062, 0.1258925411794175, 0.1584893192461124, 0.1995262314968893, 0.25118864315095973, 0.3162277660168402, 0.3981071705535002, 0.5011872336272761, 0.6309573444801981, 0.7943282347242878, 1.0000000000000082, 1.2589254117941764, 1.5848931924611271, 1.9952623149688993, 2.511886431509603, 3.1622776601684053, 3.98107170553501, 5.011872336272776, 6.309573444801995, 7.943282347242887, 10.000000000000103, 12.589254117941817, 15.848931924611303, 19.952623149688993, 25.11886431509608, 31.62277660168418, 39.81071705535018, 50.11872336272776, 63.095734448020075, 79.43282347242919, 100.00000000000122, 125.89254117941816, 158.48931924611335, 199.52623149689074, 251.1886431509613, 316.2277660168418, 398.1071705535026, 501.1872336272797, 630.957344480202, 794.3282347242919, 1000.0000000000143, 1258.9254117941869, 1584.8931924611368, 1995.2623149689073, 2511.886431509618, 3162.277660168431, 3981.071705535034, 5011.872336272797, 6309.573444802033, 7943.282347242952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge optimal lambda = 0.03162277660168396\n",
      "\n",
      "              Coef.\n",
      "METALS   -0.061364\n",
      "US_STK   -0.016407\n",
      "X10Y_TBY  0.015374\n",
      "X13W_TB   0.001258\n",
      "EURUSD   -0.012517\n",
      "\n",
      " Ridge_R^2 0.12384290357405481\n",
      "Ridge_MSE 0.003780479486607714\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "# generate a sequence of lambdas to try\n",
    "lambdas = [np.power(10, i) for i in np.arange(-4, 4, 0.1)]\n",
    "alphas = lambdas\n",
    "\n",
    "print(\"lambdas =\", lambdas)\n",
    "\n",
    "# Scale\n",
    "# train_x_scale = scale(train_x) #In case you want to scale the variables.\n",
    "\n",
    "# Use 10-fold Cross Validation to find optimal lambda and then fit the model\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=10, scoring=\"neg_mean_squared_error\").fit(train_x, train_y)\n",
    "print(\"Ridge optimal lambda =\", ridge_cv.alpha_)\n",
    "\n",
    "# Build final ridge regression model\n",
    "ridge_final = Ridge(alpha=ridge_cv.alpha_, fit_intercept=True).fit(train_x, train_y)\n",
    "# ridge_final.fit(train_x, train_y)\n",
    "\n",
    "# Print coefficients\n",
    "# print('Intercept:', ridge_final.intercept_)\n",
    "print(\n",
    "    \"\\n\",\n",
    "    pd.DataFrame(\n",
    "        (ridge_final.coef_),\n",
    "        index=[\"METALS\", \"US_STK\", \"X10Y_TBY\", \"X13W_TB\", \"EURUSD\"],\n",
    "        columns=[\"Coef.\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# R squared formula and mean squared error\n",
    "ridge_pred = ridge_final.predict(test_x)\n",
    "ridge_actual = test.DXY\n",
    "ridge_rss = np.sum(np.power(ridge_pred - ridge_actual, 2))\n",
    "ridge_tss = np.sum(np.power(ridge_actual - np.mean(ridge_actual), 2))\n",
    "ridge_rsq = 1 - ridge_rss / ridge_tss\n",
    "print(\"\\n Ridge_R^2\", ridge_rsq)\n",
    "\n",
    "ridge_MSE = np.sqrt(ridge_rss / test.shape[0])\n",
    "print(\"Ridge_MSE\", ridge_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LASSO Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso optimal lambda = 1.5848931924612534e-05\n",
      "\n",
      "              Coef.\n",
      "METALS   -0.059688\n",
      "US_STK   -0.000000\n",
      "X10Y_TBY  0.000000\n",
      "X13W_TB   0.000000\n",
      "EURUSD   -0.000000\n",
      "\n",
      " LASSO_R^2:  0.14151036558996766\n",
      "LASSO_MSE:  0.00374216922006709\n"
     ]
    }
   ],
   "source": [
    "# LASSO Regression\n",
    "\n",
    "# generate a sequence of lambdas to try\n",
    "lambdas = [np.power(10, i) for i in np.arange(6, -6, -0.1)]\n",
    "\n",
    "# Compile and fit model\n",
    "lasso_cv = LassoCV(cv=10, alphas=lambdas).fit(train_x, train_y)\n",
    "print(\"Lasso optimal lambda =\", lasso_cv.alpha_)\n",
    "# lasso_cv.fit(train_x, train_y)  # Fit Model\n",
    "\n",
    "# Scale\n",
    "# train_x_scale = scale(train_x) #In case you want to scale the variables.\n",
    "\n",
    "# Build final LASSO regression model\n",
    "lasso_final = Lasso(alpha=lasso_cv.alpha_, fit_intercept=True)\n",
    "lasso_final.fit(train_x, train_y)\n",
    "\n",
    "# Print results\n",
    "# print('Intercept:', lasso_final.intercept_)\n",
    "print(\n",
    "    \"\\n\",\n",
    "    pd.DataFrame(\n",
    "        (lasso_final.coef_),\n",
    "        index=[\"METALS\", \"US_STK\", \"X10Y_TBY\", \"X13W_TB\", \"EURUSD\"],\n",
    "        columns=[\"Coef.\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# R squared formula and mean squared error\n",
    "lasso_pred = lasso_final.predict(test_x)\n",
    "lasso_actual = test.DXY\n",
    "lasso_rss = np.sum(np.power(lasso_pred - lasso_actual, 2))\n",
    "lasso_tss = np.sum(np.power(lasso_actual - np.mean(lasso_actual), 2))\n",
    "lasso_rsq = 1 - lasso_rss / lasso_tss\n",
    "print(\"\\n LASSO_R^2: \", lasso_rsq)\n",
    "\n",
    "lasso_MSE = np.sqrt(lasso_rss / test.shape[0])\n",
    "print(\"LASSO_MSE: \", lasso_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **All of the results in one table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>Lasso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>METALS</th>\n",
       "      <td>-0.070259</td>\n",
       "      <td>-0.061364</td>\n",
       "      <td>-0.059688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_STK</th>\n",
       "      <td>-0.058324</td>\n",
       "      <td>-0.016407</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X10Y_TBY</th>\n",
       "      <td>0.019521</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X13W_TB</th>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURUSD</th>\n",
       "      <td>-0.087070</td>\n",
       "      <td>-0.012517</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R sq</th>\n",
       "      <td>0.106332</td>\n",
       "      <td>0.123843</td>\n",
       "      <td>0.141510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Sq. Err</th>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.003742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   OLS     Ridge     Lasso\n",
       "METALS       -0.070259 -0.061364 -0.059688\n",
       "US_STK       -0.058324 -0.016407 -0.000000\n",
       "X10Y_TBY      0.019521  0.015374  0.000000\n",
       "X13W_TB       0.000696  0.001258  0.000000\n",
       "EURUSD       -0.087070 -0.012517 -0.000000\n",
       "R sq          0.106332  0.123843  0.141510\n",
       "Mean Sq. Err  0.003818  0.003780  0.003742"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All of the results in one table\n",
    "\n",
    "OLS_df = pd.DataFrame(ols_final.summary2().tables[1][\"Coef.\"]).rename(\n",
    "    columns={\"Coef.\": \"OLS\"}\n",
    ")\n",
    "\n",
    "Ridge_df = pd.DataFrame(\n",
    "    np.insert(ridge_final.coef_, 0, ridge_final.intercept_),\n",
    "    index=[\"Intercept\", \"METALS\", \"US_STK\", \"X10Y_TBY\", \"X13W_TB\", \"EURUSD\"],\n",
    "    columns=[\"Ridge\"],\n",
    ")\n",
    "\n",
    "Lasso_df = pd.DataFrame(\n",
    "    np.insert(lasso_final.coef_, 0, lasso_final.intercept_),\n",
    "    index=[\"Intercept\", \"METALS\", \"US_STK\", \"X10Y_TBY\", \"X13W_TB\", \"EURUSD\"],\n",
    "    columns=[\"Lasso\"],\n",
    ")\n",
    "\n",
    "df = OLS_df.merge(Ridge_df, left_index=True, right_index=True)\n",
    "df = df.merge(Lasso_df, left_index=True, right_index=True)\n",
    "\n",
    "pd.concat([df, pd.DataFrame({\"OLS\": [ols_rsq, ols_MSE], \"Ridge\": [ridge_rsq, ridge_MSE], \"Lasso\": [lasso_rsq, lasso_MSE]}, index=[\"R sq\", \"Mean Sq. Err\"])], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3 shows the model results from our three methods. The table includes coefficients, test $R^2$ and test mean squared error for OLS, ridge and lasso regressions. From Figure 3, we can see lasso has the best $R^2$ for the testing dataset, followed by ridge with OLS being the worst. We also notice that there is only one independent variable, *METALS*, which has a non-0 coefficient. Lasso only selects *METALS* for the regression model. Lasso makes the model very simple, yet it has the best fit for testing data. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
